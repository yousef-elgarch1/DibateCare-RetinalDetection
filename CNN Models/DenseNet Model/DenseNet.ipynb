{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qj3Fgd1zsf4Z"
      },
      "outputs": [],
      "source": [
        "# DenseNet Model for Diabetic Retinopathy Detection\n",
        "# Google Colab Notebook - Run each cell individually\n",
        "\n",
        "# ========== CELL 1: Install Dependencies ==========\n",
        "!pip install opencv-python-headless\n",
        "!pip install scikit-learn\n",
        "!pip install seaborn\n",
        "!pip install plotly\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "print(\"‚úÖ Dependencies installed successfully!\")\n",
        "\n",
        "# ========== CELL 2: Import Libraries ==========\n",
        "import os\n",
        "import zipfile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "import cv2\n",
        "from google.colab import files, drive\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# ========== CELL 3: Mount Google Drive (Optional) ==========\n",
        "# Uncomment if you want to save/load from Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "print(\"üíæ Google Drive mounting available if needed\")\n",
        "\n",
        "# ========== CELL 4: Upload Dataset ==========\n",
        "print(\"üìÅ Please upload your diabetic retinopathy dataset zip file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "zip_filename = list(uploaded.keys())[0]\n",
        "print(f\"üì¶ Extracting {zip_filename}...\")\n",
        "\n",
        "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/diabetic_retinopathy_data')\n",
        "\n",
        "print(\"‚úÖ Dataset extracted to: /content/diabetic_retinopathy_data\")\n",
        "\n",
        "# ========== CELL 5: Dataset Preprocessing Class ==========\n",
        "class RetinalImagePreprocessor:\n",
        "    def __init__(self, target_size=224):  # DenseNet uses 224x224\n",
        "        self.target_size = target_size\n",
        "\n",
        "    def crop_image_from_gray(self, img, tol=7):\n",
        "        \"\"\"Crop image to remove black borders\"\"\"\n",
        "        if img.ndim == 2:\n",
        "            mask = img > tol\n",
        "            return img[np.ix_(mask.any(1), mask.any(0))]\n",
        "        elif img.ndim == 3:\n",
        "            gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "            mask = gray_img > tol\n",
        "            check_shape = img[:,:,0][np.ix_(mask.any(1), mask.any(0))].shape[0]\n",
        "            if (check_shape == 0):\n",
        "                return img\n",
        "            else:\n",
        "                img1 = img[:,:,0][np.ix_(mask.any(1), mask.any(0))]\n",
        "                img2 = img[:,:,1][np.ix_(mask.any(1), mask.any(0))]\n",
        "                img3 = img[:,:,2][np.ix_(mask.any(1), mask.any(0))]\n",
        "                img = np.stack([img1, img2, img3], axis=-1)\n",
        "        return img\n",
        "\n",
        "    def apply_clahe(self, image):\n",
        "        \"\"\"Apply CLAHE for contrast enhancement\"\"\"\n",
        "        lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
        "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
        "        lab[:,:,0] = clahe.apply(lab[:,:,0])\n",
        "        return cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
        "\n",
        "    def preprocess_retinal_image(self, image_path):\n",
        "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = self.crop_image_from_gray(image)\n",
        "        image = self.apply_clahe(image)\n",
        "        image = cv2.resize(image, (self.target_size, self.target_size))\n",
        "        return image\n",
        "\n",
        "print(\"‚úÖ Preprocessing class defined!\")\n",
        "\n",
        "# ========== CELL 6: Dataset Organization ==========\n",
        "def organize_dataset(base_path):\n",
        "    \"\"\"Organize dataset from folder structure\"\"\"\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    label_names = ['No_DR', 'Mild', 'Moderate', 'Severe', 'Proliferative_DR']\n",
        "\n",
        "    for root, dirs, files in os.walk(base_path):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                file_path = os.path.join(root, file)\n",
        "                folder_name = os.path.basename(root)\n",
        "\n",
        "                label = -1\n",
        "                for idx, label_name in enumerate(label_names):\n",
        "                    if label_name.lower() in folder_name.lower() or label_name.lower() in file.lower():\n",
        "                        label = idx\n",
        "                        break\n",
        "\n",
        "                if label != -1:\n",
        "                    image_paths.append(file_path)\n",
        "                    labels.append(label)\n",
        "\n",
        "    return image_paths, labels, label_names\n",
        "\n",
        "# Organize the dataset\n",
        "image_paths, labels, label_names = organize_dataset('/content/diabetic_retinopathy_data')\n",
        "\n",
        "print(f\"‚úÖ Found {len(image_paths)} images\")\n",
        "print(\"üìä Class distribution:\", Counter(labels))\n",
        "print(\"üè∑Ô∏è Classes:\", label_names)\n",
        "\n",
        "# ========== CELL 7: Custom Dataset Class ==========\n",
        "class DiabeticRetinopathyDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None, preprocessor=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.preprocessor = preprocessor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.preprocessor:\n",
        "            image = self.preprocessor.preprocess_retinal_image(image_path)\n",
        "            image = Image.fromarray(image)\n",
        "        else:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "print(\"‚úÖ Custom Dataset class defined!\")\n",
        "\n",
        "# ========== CELL 8: Data Transforms ==========\n",
        "# Training transforms with augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # DenseNet input size\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.RandomVerticalFlip(0.2),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.1),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Validation/Test transforms without augmentation\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "print(\"‚úÖ Data transforms defined!\")\n",
        "\n",
        "# ========== CELL 9: Train-Validation-Test Split ==========\n",
        "# Split dataset\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    image_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.125, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(f\"üìä Dataset Split:\")\n",
        "print(f\"   Train samples: {len(X_train)} ({len(X_train)/len(image_paths)*100:.1f}%)\")\n",
        "print(f\"   Validation samples: {len(X_val)} ({len(X_val)/len(image_paths)*100:.1f}%)\")\n",
        "print(f\"   Test samples: {len(X_test)} ({len(X_test)/len(image_paths)*100:.1f}%)\")\n",
        "\n",
        "# ========== CELL 10: Create Data Loaders ==========\n",
        "# Create preprocessor\n",
        "preprocessor = RetinalImagePreprocessor(target_size=224)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = DiabeticRetinopathyDataset(X_train, y_train, train_transform, preprocessor)\n",
        "val_dataset = DiabeticRetinopathyDataset(X_val, y_val, val_transform, preprocessor)\n",
        "test_dataset = DiabeticRetinopathyDataset(X_test, y_test, val_transform, preprocessor)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 16  # DenseNet is memory intensive\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(\"‚úÖ Data loaders created successfully!\")\n",
        "print(f\"   Batch size: {batch_size}\")\n",
        "print(f\"   Train batches: {len(train_loader)}\")\n",
        "print(f\"   Validation batches: {len(val_loader)}\")\n",
        "print(f\"   Test batches: {len(test_loader)}\")\n",
        "\n",
        "# ========== CELL 11: Visualize Sample Images ==========\n",
        "def visualize_samples(data_loader, label_names, num_samples=8):\n",
        "    dataiter = iter(data_loader)\n",
        "    images, labels = next(dataiter)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    for i in range(min(num_samples, len(images))):\n",
        "        image = images[i]\n",
        "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "        image = image * std + mean\n",
        "        image = torch.clamp(image, 0, 1)\n",
        "\n",
        "        image_np = image.permute(1, 2, 0).numpy()\n",
        "\n",
        "        axes[i].imshow(image_np)\n",
        "        axes[i].set_title(f'Class: {label_names[labels[i]]}', fontsize=12, fontweight='bold')\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.suptitle('Sample Images from Dataset', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize samples\n",
        "visualize_samples(train_loader, label_names)\n",
        "\n",
        "# ========== CELL 12: Plot Class Distribution ==========\n",
        "def plot_class_distribution(data_loader, label_names, title=\"Class Distribution\"):\n",
        "    all_labels = []\n",
        "    for _, labels in data_loader:\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    unique_labels, counts = np.unique(all_labels, return_counts=True)\n",
        "\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, len(label_names)))\n",
        "    bars = plt.bar([label_names[i] for i in unique_labels], counts, color=colors)\n",
        "\n",
        "    plt.title(title, fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Diabetic Retinopathy Stage')\n",
        "    plt.ylabel('Number of Images')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, count in zip(bars, counts):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "                str(count), ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print statistics\n",
        "    print(f\"\\nüìä {title}:\")\n",
        "    for i, count in zip(unique_labels, counts):\n",
        "        percentage = (count / len(all_labels)) * 100\n",
        "        print(f\"   {label_names[i]}: {count} images ({percentage:.1f}%)\")\n",
        "\n",
        "# Plot class distribution\n",
        "plot_class_distribution(train_loader, label_names, \"Training Set Class Distribution\")\n",
        "\n",
        "# ========== CELL 13: DenseNet Model ==========\n",
        "class DenseNetDiabeticRetinopathy(nn.Module):\n",
        "    def __init__(self, num_classes=5, model_size='121', pretrained=True, dropout_rate=0.4):\n",
        "        super(DenseNetDiabeticRetinopathy, self).__init__()\n",
        "\n",
        "        # Choose DenseNet variant\n",
        "        if model_size == '121':\n",
        "            if pretrained:\n",
        "                self.backbone = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
        "            else:\n",
        "                self.backbone = models.densenet121(weights=None)\n",
        "            feature_dim = 1024\n",
        "        elif model_size == '169':\n",
        "            if pretrained:\n",
        "                self.backbone = models.densenet169(weights=models.DenseNet169_Weights.IMAGENET1K_V1)\n",
        "            else:\n",
        "                self.backbone = models.densenet169(weights=None)\n",
        "            feature_dim = 1664\n",
        "        elif model_size == '201':\n",
        "            if pretrained:\n",
        "                self.backbone = models.densenet201(weights=models.DenseNet201_Weights.IMAGENET1K_V1)\n",
        "            else:\n",
        "                self.backbone = models.densenet201(weights=None)\n",
        "            feature_dim = 1920\n",
        "\n",
        "        # Remove the original classifier\n",
        "        self.backbone.classifier = nn.Identity()\n",
        "\n",
        "        # Add channel attention mechanism\n",
        "        self.channel_attention = nn.Sequential(\n",
        "            nn.Linear(feature_dim, feature_dim // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(feature_dim // 4, feature_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Custom classifier with multiple branches\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(feature_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(dropout_rate * 0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "        # Auxiliary classifier for feature learning\n",
        "        self.aux_classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(feature_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in [self.channel_attention, self.classifier, self.aux_classifier]:\n",
        "            for layer in m.modules():\n",
        "                if isinstance(layer, nn.Linear):\n",
        "                    nn.init.xavier_uniform_(layer.weight)\n",
        "                    nn.init.constant_(layer.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features\n",
        "        features = self.backbone(x)\n",
        "\n",
        "        # Apply channel attention\n",
        "        attention_weights = self.channel_attention(features)\n",
        "        attended_features = features * attention_weights\n",
        "\n",
        "        # Main classification\n",
        "        main_output = self.classifier(attended_features)\n",
        "\n",
        "        # Auxiliary classification (for training regularization)\n",
        "        aux_output = self.aux_classifier(features)\n",
        "\n",
        "        if self.training:\n",
        "            return main_output, aux_output\n",
        "        else:\n",
        "            return main_output\n",
        "\n",
        "print(\"‚úÖ DenseNet Model defined!\")\n",
        "\n",
        "# ========== CELL 14: Initialize Model and Training Setup ==========\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üîß Using device: {device}\")\n",
        "\n",
        "# Initialize model (you can change to '121', '169', or '201')\n",
        "model = DenseNetDiabeticRetinopathy(num_classes=len(label_names), model_size='121', pretrained=True, dropout_rate=0.4)\n",
        "model = model.to(device)\n",
        "\n",
        "# Mixed precision training setup\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, epochs=40, steps_per_epoch=len(train_loader))\n",
        "\n",
        "print(\"‚úÖ Model initialized and moved to device!\")\n",
        "print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"üìä Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "print(\"üöÄ Mixed precision training enabled!\")\n",
        "\n",
        "# ========== CELL 15: Training Function ==========\n",
        "def train_epoch(model, train_loader, criterion, optimizer, scheduler, scaler, device, aux_weight=0.3):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            if model.training:\n",
        "                main_output, aux_output = model(data)\n",
        "                main_loss = criterion(main_output, targets)\n",
        "                aux_loss = criterion(aux_output, targets)\n",
        "                loss = main_loss + aux_weight * aux_loss\n",
        "                outputs = main_output\n",
        "            else:\n",
        "                outputs = model(data)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if batch_idx % 20 == 0:\n",
        "            print(f'   Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}, LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "\n",
        "    return total_loss / len(train_loader), 100. * correct / total\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    all_probabilities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, targets in val_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(data)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "            all_probabilities.extend(F.softmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "    return (total_loss / len(val_loader), 100. * correct / total,\n",
        "            all_predictions, all_targets, all_probabilities)\n",
        "\n",
        "print(\"‚úÖ Training functions defined!\")\n",
        "\n",
        "# ========== CELL 16: Training Loop ==========\n",
        "num_epochs = 40\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "learning_rates = []\n",
        "\n",
        "best_val_acc = 0\n",
        "\n",
        "print(\"üöÄ Starting training with mixed precision...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "\n",
        "    # Training\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, scheduler, scaler, device)\n",
        "\n",
        "    # Validation\n",
        "    val_loss, val_acc, val_preds, val_targets, val_probs = validate_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    # Store metrics\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "    learning_rates.append(current_lr)\n",
        "\n",
        "    print(f'   Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "    print(f'   Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "    print(f'   Learning Rate: {current_lr:.6f}')\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), '/content/best_densenet_model.pth')\n",
        "        print(f'   ‚úÖ New best model saved! Validation Accuracy: {val_acc:.2f}%')\n",
        "\n",
        "    print('-' * 60)\n",
        "\n",
        "print(\"üéâ Training completed!\")\n",
        "\n",
        "# ========== CELL 17: Plot Training History ==========\n",
        "def plot_training_history(train_losses, val_losses, train_accuracies, val_accuracies, learning_rates):\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    # Loss curves\n",
        "    axes[0,0].plot(epochs, train_losses, 'b-', label='Train Loss', linewidth=2)\n",
        "    axes[0,0].plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
        "    axes[0,0].set_title('Training and Validation Loss', fontweight='bold')\n",
        "    axes[0,0].set_xlabel('Epoch')\n",
        "    axes[0,0].set_ylabel('Loss')\n",
        "    axes[0,0].legend()\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Accuracy curves\n",
        "    axes[0,1].plot(epochs, train_accuracies, 'b-', label='Train Accuracy', linewidth=2)\n",
        "    axes[0,1].plot(epochs, val_accuracies, 'r-', label='Validation Accuracy', linewidth=2)\n",
        "    axes[0,1].set_title('Training and Validation Accuracy', fontweight='bold')\n",
        "    axes[0,1].set_xlabel('Epoch')\n",
        "    axes[0,1].set_ylabel('Accuracy (%)')\n",
        "    axes[0,1].legend()\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Learning rate\n",
        "    axes[1,0].plot(epochs, learning_rates, 'g-', linewidth=2)\n",
        "    axes[1,0].set_title('OneCycleLR Schedule', fontweight='bold')\n",
        "    axes[1,0].set_xlabel('Epoch')\n",
        "    axes[1,0].set_ylabel('Learning Rate')\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Overfitting indicator\n",
        "    loss_diff = np.array(val_losses) - np.array(train_losses)\n",
        "    axes[1,1].plot(epochs, loss_diff, 'purple', linewidth=2)\n",
        "    axes[1,1].set_title('Overfitting Indicator (Val Loss - Train Loss)', fontweight='bold')\n",
        "    axes[1,1].set_xlabel('Epoch')\n",
        "    axes[1,1].set_ylabel('Loss Difference')\n",
        "    axes[1,1].grid(True, alpha=0.3)\n",
        "    axes[1,1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot training history\n",
        "plot_training_history(train_losses, val_losses, train_accuracies, val_accuracies, learning_rates)\n",
        "\n",
        "# ========== CELL 18: Test Set Evaluation ==========\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load('/content/best_densenet_model.pth'))\n",
        "print(\"‚úÖ Best model loaded for testing\")\n",
        "\n",
        "# Test evaluation\n",
        "test_loss, test_acc, test_preds, test_targets, test_probs = validate_epoch(model, test_loader, criterion, device)\n",
        "\n",
        "print(\"üéØ Test Set Results:\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "# ========== CELL 19: Confusion Matrix ==========\n",
        "def plot_confusion_matrix(y_true, y_pred, label_names):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Absolute confusion matrix\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "               xticklabels=label_names, yticklabels=label_names)\n",
        "    plt.title('Confusion Matrix (Absolute)', fontweight='bold')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "\n",
        "    # Normalized confusion matrix\n",
        "    plt.subplot(1, 2, 2)\n",
        "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues',\n",
        "               xticklabels=label_names, yticklabels=label_names)\n",
        "    plt.title('Confusion Matrix (Normalized)', fontweight='bold')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(test_targets, test_preds, label_names)\n",
        "\n",
        "# ========== CELL 20: ROC Curves ==========\n",
        "def plot_roc_curves(y_true, y_probs, label_names):\n",
        "    from sklearn.preprocessing import label_binarize\n",
        "\n",
        "    y_true_bin = label_binarize(y_true, classes=range(len(label_names)))\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    colors = plt.cm.Set1(np.linspace(0, 1, len(label_names)))\n",
        "\n",
        "    for i in range(len(label_names)):\n",
        "        if len(np.unique(y_true_bin[:, i])) > 1:\n",
        "            fpr, tpr, _ = roc_curve(y_true_bin[:, i], np.array(y_probs)[:, i])\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "            plt.plot(fpr, tpr, color=colors[i], lw=2,\n",
        "                    label=f'{label_names[i]} (AUC = {roc_auc:.3f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Multi-class ROC Curves - DenseNet Model', fontweight='bold')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "# Plot ROC curves\n",
        "plot_roc_curves(test_targets, test_probs, label_names)\n",
        "\n",
        "# ========== CELL 21: Classification Report ==========\n",
        "print(\"üìä Detailed Classification Report:\")\n",
        "print(\"=\" * 60)\n",
        "report = classification_report(test_targets, test_preds, target_names=label_names, digits=3)\n",
        "print(report)\n",
        "\n",
        "# Per-class metrics\n",
        "report_dict = classification_report(test_targets, test_preds, target_names=label_names, output_dict=True)\n",
        "\n",
        "print(\"\\nüìà Per-Class Detailed Metrics:\")\n",
        "print(\"-\" * 60)\n",
        "for class_name in label_names:\n",
        "    if class_name in report_dict:\n",
        "        precision = report_dict[class_name]['precision']\n",
        "        recall = report_dict[class_name]['recall']\n",
        "        f1 = report_dict[class_name]['f1-score']\n",
        "        support = report_dict[class_name]['support']\n",
        "        print(f\"{class_name:15} | Precision: {precision:.3f} | Recall: {recall:.3f} | \"\n",
        "              f\"F1: {f1:.3f} | Support: {support}\")\n",
        "\n",
        "print(f\"\\nüéØ Overall Metrics:\")\n",
        "print(f\"   Accuracy: {report_dict['accuracy']:.3f}\")\n",
        "print(f\"   Macro Avg F1: {report_dict['macro avg']['f1-score']:.3f}\")\n",
        "print(f\"   Weighted Avg F1: {report_dict['weighted avg']['f1-score']:.3f}\")\n",
        "\n",
        "# ========== CELL 22: Save Model ==========\n",
        "# Save final model\n",
        "torch.save(model.state_dict(), '/content/densenet_diabetic_retinopathy.pth')\n",
        "print(\"üíæ Model saved as: densenet_diabetic_retinopathy.pth\")\n",
        "\n",
        "# Save training history\n",
        "training_history = {\n",
        "    'train_losses': train_losses,\n",
        "    'val_losses': val_losses,\n",
        "    'train_accuracies': train_accuracies,\n",
        "    'val_accuracies': val_accuracies,\n",
        "    'learning_rates': learning_rates,\n",
        "    'test_accuracy': test_acc,\n",
        "    'test_loss': test_loss\n",
        "}\n",
        "\n",
        "import pickle\n",
        "with open('/content/densenet_training_history.pkl', 'wb') as f:\n",
        "    pickle.dump(training_history, f)\n",
        "\n",
        "print(\"üìà Training history saved!\")\n",
        "\n",
        "# ========== CELL 23: Download Results ==========\n",
        "# Download model and results\n",
        "from google.colab import files\n",
        "\n",
        "print(\"‚¨áÔ∏è Downloading model and results...\")\n",
        "files.download('/content/densenet_diabetic_retinopathy.pth')\n",
        "files.download('/content/densenet_training_history.pkl')\n",
        "\n",
        "print(\"‚úÖ DenseNet Model Training Complete!\")\n",
        "print(f\"üéØ Final Test Accuracy: {test_acc:.2f}%\")\n",
        "print(\"üéâ All files downloaded successfully!\")"
      ]
    }
  ]
}