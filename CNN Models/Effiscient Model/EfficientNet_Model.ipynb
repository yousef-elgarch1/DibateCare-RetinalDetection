{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqfgYC-SsSFs"
      },
      "outputs": [],
      "source": [
        "# EfficientNet Model for Diabetic Retinopathy Detection\n",
        "# Google Colab Notebook - Run each cell individually\n",
        "\n",
        "# ========== CELL 1: Install Dependencies ==========\n",
        "!pip install opencv-python-headless\n",
        "!pip install scikit-learn\n",
        "!pip install seaborn\n",
        "!pip install plotly\n",
        "!pip install efficientnet-pytorch\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "print(\"âœ… Dependencies installed successfully!\")\n",
        "\n",
        "# ========== CELL 2: Import Libraries ==========\n",
        "import os\n",
        "import zipfile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "import cv2\n",
        "from google.colab import files, drive\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "print(\"âœ… All libraries imported successfully!\")\n",
        "print(f\"\\nðŸŽ¯ Overall Metrics:\")\n",
        "print(f\"   Accuracy: {report_dict['accuracy']:.3f}\")\n",
        "print(f\"   Macro Avg F1: {report_dict['macro avg']['f1-score']:.3f}\")\n",
        "print(f\"   Weighted Avg F1: {report_dict['weighted avg']['f1-score']:.3f}\")\n",
        "\n",
        "# ========== CELL 22: Save Model ==========\n",
        "# Save final model\n",
        "torch.save(model.state_dict(), '/content/efficientnet_diabetic_retinopathy.pth')\n",
        "print(\"ðŸ’¾ Model saved as: efficientnet_diabetic_retinopathy.pth\")\n",
        "\n",
        "# Save training history\n",
        "training_history = {\n",
        "    'train_losses': train_losses,\n",
        "    'val_losses': val_losses,\n",
        "    'train_accuracies': train_accuracies,\n",
        "    'val_accuracies': val_accuracies,\n",
        "    'learning_rates': learning_rates,\n",
        "    'test_accuracy': test_acc,\n",
        "    'test_loss': test_loss\n",
        "}\n",
        "\n",
        "import pickle\n",
        "with open('/content/efficientnet_training_history.pkl', 'wb') as f:\n",
        "    pickle.dump(training_history, f)\n",
        "\n",
        "print(\"ðŸ“ˆ Training history saved!\")\n",
        "\n",
        "# ========== CELL 23: Download Results ==========\n",
        "# Download model and results\n",
        "from google.colab import files\n",
        "\n",
        "print(\"â¬‡ï¸ Downloading model and results...\")\n",
        "files.download('/content/efficientnet_diabetic_retinopathy.pth')\n",
        "files.download('/content/efficientnet_training_history.pkl')\n",
        "\n",
        "print(\"âœ… EfficientNet Model Training Complete!\")\n",
        "print(f\"ðŸŽ¯ Final Test Accuracy: {test_acc:.2f}%\")\n",
        "print(\"ðŸŽ‰ All files downloaded successfully!\")\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# ========== CELL 3: Mount Google Drive (Optional) ==========\n",
        "# Uncomment if you want to save/load from Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "print(\"ðŸ’¾ Google Drive mounting available if needed\")\n",
        "\n",
        "# ========== CELL 4: Upload Dataset ==========\n",
        "print(\"ðŸ“ Please upload your diabetic retinopathy dataset zip file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "zip_filename = list(uploaded.keys())[0]\n",
        "print(f\"ðŸ“¦ Extracting {zip_filename}...\")\n",
        "\n",
        "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/diabetic_retinopathy_data')\n",
        "\n",
        "print(\"âœ… Dataset extracted to: /content/diabetic_retinopathy_data\")\n",
        "\n",
        "# ========== CELL 5: Dataset Preprocessing Class ==========\n",
        "class RetinalImagePreprocessor:\n",
        "    def __init__(self, target_size=224):  # EfficientNet B0 uses 224x224\n",
        "        self.target_size = target_size\n",
        "\n",
        "    def crop_image_from_gray(self, img, tol=7):\n",
        "        \"\"\"Crop image to remove black borders\"\"\"\n",
        "        if img.ndim == 2:\n",
        "            mask = img > tol\n",
        "            return img[np.ix_(mask.any(1), mask.any(0))]\n",
        "        elif img.ndim == 3:\n",
        "            gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "            mask = gray_img > tol\n",
        "            check_shape = img[:,:,0][np.ix_(mask.any(1), mask.any(0))].shape[0]\n",
        "            if (check_shape == 0):\n",
        "                return img\n",
        "            else:\n",
        "                img1 = img[:,:,0][np.ix_(mask.any(1), mask.any(0))]\n",
        "                img2 = img[:,:,1][np.ix_(mask.any(1), mask.any(0))]\n",
        "                img3 = img[:,:,2][np.ix_(mask.any(1), mask.any(0))]\n",
        "                img = np.stack([img1, img2, img3], axis=-1)\n",
        "        return img\n",
        "\n",
        "    def apply_clahe(self, image):\n",
        "        \"\"\"Apply CLAHE for contrast enhancement\"\"\"\n",
        "        lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)\n",
        "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
        "        lab[:,:,0] = clahe.apply(lab[:,:,0])\n",
        "        return cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
        "\n",
        "    def preprocess_retinal_image(self, image_path):\n",
        "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = self.crop_image_from_gray(image)\n",
        "        image = self.apply_clahe(image)\n",
        "        image = cv2.resize(image, (self.target_size, self.target_size))\n",
        "        return image\n",
        "\n",
        "print(\"âœ… Preprocessing class defined!\")\n",
        "\n",
        "# ========== CELL 6: Dataset Organization ==========\n",
        "def organize_dataset(base_path):\n",
        "    \"\"\"Organize dataset from folder structure\"\"\"\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    label_names = ['No_DR', 'Mild', 'Moderate', 'Severe', 'Proliferative_DR']\n",
        "\n",
        "    for root, dirs, files in os.walk(base_path):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                file_path = os.path.join(root, file)\n",
        "                folder_name = os.path.basename(root)\n",
        "\n",
        "                label = -1\n",
        "                for idx, label_name in enumerate(label_names):\n",
        "                    if label_name.lower() in folder_name.lower() or label_name.lower() in file.lower():\n",
        "                        label = idx\n",
        "                        break\n",
        "\n",
        "                if label != -1:\n",
        "                    image_paths.append(file_path)\n",
        "                    labels.append(label)\n",
        "\n",
        "    return image_paths, labels, label_names\n",
        "\n",
        "# Organize the dataset\n",
        "image_paths, labels, label_names = organize_dataset('/content/diabetic_retinopathy_data')\n",
        "\n",
        "print(f\"âœ… Found {len(image_paths)} images\")\n",
        "print(\"ðŸ“Š Class distribution:\", Counter(labels))\n",
        "print(\"ðŸ·ï¸ Classes:\", label_names)\n",
        "\n",
        "# ========== CELL 7: Custom Dataset Class ==========\n",
        "class DiabeticRetinopathyDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None, preprocessor=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.preprocessor = preprocessor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.preprocessor:\n",
        "            image = self.preprocessor.preprocess_retinal_image(image_path)\n",
        "            image = Image.fromarray(image)\n",
        "        else:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "print(\"âœ… Custom Dataset class defined!\")\n",
        "\n",
        "# ========== CELL 8: Data Transforms ==========\n",
        "# Training transforms with augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # EfficientNet B0 input size\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.RandomVerticalFlip(0.2),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.1),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Validation/Test transforms without augmentation\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "print(\"âœ… Data transforms defined!\")\n",
        "\n",
        "# ========== CELL 9: Train-Validation-Test Split ==========\n",
        "# Split dataset\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "    image_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.125, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(f\"ðŸ“Š Dataset Split:\")\n",
        "print(f\"   Train samples: {len(X_train)} ({len(X_train)/len(image_paths)*100:.1f}%)\")\n",
        "print(f\"   Validation samples: {len(X_val)} ({len(X_val)/len(image_paths)*100:.1f}%)\")\n",
        "print(f\"   Test samples: {len(X_test)} ({len(X_test)/len(image_paths)*100:.1f}%)\")\n",
        "\n",
        "# ========== CELL 10: Create Data Loaders ==========\n",
        "# Create preprocessor\n",
        "preprocessor = RetinalImagePreprocessor(target_size=224)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = DiabeticRetinopathyDataset(X_train, y_train, train_transform, preprocessor)\n",
        "val_dataset = DiabeticRetinopathyDataset(X_val, y_val, val_transform, preprocessor)\n",
        "test_dataset = DiabeticRetinopathyDataset(X_test, y_test, val_transform, preprocessor)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 32  # EfficientNet can handle medium batches efficiently\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(\"âœ… Data loaders created successfully!\")\n",
        "print(f\"   Batch size: {batch_size}\")\n",
        "print(f\"   Train batches: {len(train_loader)}\")\n",
        "print(f\"   Validation batches: {len(val_loader)}\")\n",
        "print(f\"   Test batches: {len(test_loader)}\")\n",
        "\n",
        "# ========== CELL 11: Visualize Sample Images ==========\n",
        "def visualize_samples(data_loader, label_names, num_samples=8):\n",
        "    dataiter = iter(data_loader)\n",
        "    images, labels = next(dataiter)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    for i in range(min(num_samples, len(images))):\n",
        "        image = images[i]\n",
        "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "        image = image * std + mean\n",
        "        image = torch.clamp(image, 0, 1)\n",
        "\n",
        "        image_np = image.permute(1, 2, 0).numpy()\n",
        "\n",
        "        axes[i].imshow(image_np)\n",
        "        axes[i].set_title(f'Class: {label_names[labels[i]]}', fontsize=12, fontweight='bold')\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.suptitle('Sample Images from Dataset', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize samples\n",
        "visualize_samples(train_loader, label_names)\n",
        "\n",
        "# ========== CELL 12: Plot Class Distribution ==========\n",
        "def plot_class_distribution(data_loader, label_names, title=\"Class Distribution\"):\n",
        "    all_labels = []\n",
        "    for _, labels in data_loader:\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    unique_labels, counts = np.unique(all_labels, return_counts=True)\n",
        "\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, len(label_names)))\n",
        "    bars = plt.bar([label_names[i] for i in unique_labels], counts, color=colors)\n",
        "\n",
        "    plt.title(title, fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Diabetic Retinopathy Stage')\n",
        "    plt.ylabel('Number of Images')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, count in zip(bars, counts):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "                str(count), ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print statistics\n",
        "    print(f\"\\nðŸ“Š {title}:\")\n",
        "    for i, count in zip(unique_labels, counts):\n",
        "        percentage = (count / len(all_labels)) * 100\n",
        "        print(f\"   {label_names[i]}: {count} images ({percentage:.1f}%)\")\n",
        "\n",
        "# Plot class distribution\n",
        "plot_class_distribution(train_loader, label_names, \"Training Set Class Distribution\")\n",
        "\n",
        "# ========== CELL 13: EfficientNet Model ==========\n",
        "class EfficientNetDiabeticRetinopathy(nn.Module):\n",
        "    def __init__(self, num_classes=5, model_size='b0', pretrained=True, dropout_rate=0.3):\n",
        "        super(EfficientNetDiabeticRetinopathy, self).__init__()\n",
        "\n",
        "        # Choose EfficientNet variant\n",
        "        if model_size == 'b0':\n",
        "            if pretrained:\n",
        "                self.backbone = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "            else:\n",
        "                self.backbone = models.efficientnet_b0(weights=None)\n",
        "            feature_dim = 1280\n",
        "        elif model_size == 'b3':\n",
        "            if pretrained:\n",
        "                self.backbone = models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.IMAGENET1K_V1)\n",
        "            else:\n",
        "                self.backbone = models.efficientnet_b3(weights=None)\n",
        "            feature_dim = 1536\n",
        "        elif model_size == 'b5':\n",
        "            if pretrained:\n",
        "                self.backbone = models.efficientnet_b5(weights=models.EfficientNet_B5_Weights.IMAGENET1K_V1)\n",
        "            else:\n",
        "                self.backbone = models.efficientnet_b5(weights=None)\n",
        "            feature_dim = 2048\n",
        "\n",
        "        # Remove the original classifier\n",
        "        self.backbone.classifier = nn.Identity()\n",
        "\n",
        "        # Enhanced classifier for diabetic retinopathy\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(feature_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(dropout_rate * 0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "        # Auxiliary classifier for feature learning\n",
        "        self.aux_classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(feature_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "        # Initialize the new layers\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in [self.classifier, self.aux_classifier]:\n",
        "            for layer in m.modules():\n",
        "                if isinstance(layer, nn.Linear):\n",
        "                    nn.init.xavier_uniform_(layer.weight)\n",
        "                    nn.init.constant_(layer.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        main_output = self.classifier(features)\n",
        "        aux_output = self.aux_classifier(features)\n",
        "\n",
        "        if self.training:\n",
        "            return main_output, aux_output\n",
        "        else:\n",
        "            return main_output\n",
        "\n",
        "print(\"âœ… EfficientNet Model defined!\")\n",
        "\n",
        "# ========== CELL 14: Initialize Model and Training Setup ==========\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ðŸ”§ Using device: {device}\")\n",
        "\n",
        "# Initialize model (you can change to 'b0', 'b3', or 'b5')\n",
        "model = EfficientNetDiabeticRetinopathy(num_classes=len(label_names), model_size='b0', pretrained=True, dropout_rate=0.3)\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)\n",
        "\n",
        "print(\"âœ… Model initialized and moved to device!\")\n",
        "print(f\"ðŸ“Š Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"ðŸ“Š Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "# ========== CELL 15: Training Function ==========\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device, aux_weight=0.3):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if model.training:\n",
        "            main_output, aux_output = model(data)\n",
        "            main_loss = criterion(main_output, targets)\n",
        "            aux_loss = criterion(aux_output, targets)\n",
        "            loss = main_loss + aux_weight * aux_loss\n",
        "            outputs = main_output\n",
        "        else:\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f'   Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
        "\n",
        "    return total_loss / len(train_loader), 100. * correct / total\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    all_probabilities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, targets in val_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "            all_probabilities.extend(F.softmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "    return (total_loss / len(val_loader), 100. * correct / total,\n",
        "            all_predictions, all_targets, all_probabilities)\n",
        "\n",
        "print(\"âœ… Training functions defined!\")\n",
        "\n",
        "# ========== CELL 16: Training Loop ==========\n",
        "num_epochs = 30\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "learning_rates = []\n",
        "\n",
        "best_val_acc = 0\n",
        "patience_counter = 0\n",
        "early_stopping_patience = 10\n",
        "\n",
        "print(\"ðŸš€ Starting training...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "\n",
        "    # Training\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "    # Validation\n",
        "    val_loss, val_acc, val_preds, val_targets, val_probs = validate_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "    # Scheduler step\n",
        "    scheduler.step()\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    # Store metrics\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "    learning_rates.append(current_lr)\n",
        "\n",
        "    print(f'   Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "    print(f'   Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "    print(f'   Learning Rate: {current_lr:.6f}')\n",
        "\n",
        "    # Early stopping\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), '/content/best_efficientnet_model.pth')\n",
        "        print(f'   âœ… New best model saved! Validation Accuracy: {val_acc:.2f}%')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if patience_counter >= early_stopping_patience:\n",
        "        print(f'   â¹ï¸ Early stopping at epoch {epoch+1}')\n",
        "        break\n",
        "\n",
        "    print('-' * 60)\n",
        "\n",
        "print(\"ðŸŽ‰ Training completed!\")\n",
        "\n",
        "# ========== CELL 17: Plot Training History ==========\n",
        "def plot_training_history(train_losses, val_losses, train_accuracies, val_accuracies, learning_rates):\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    # Loss curves\n",
        "    axes[0,0].plot(epochs, train_losses, 'b-', label='Train Loss', linewidth=2)\n",
        "    axes[0,0].plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
        "    axes[0,0].set_title('Training and Validation Loss', fontweight='bold')\n",
        "    axes[0,0].set_xlabel('Epoch')\n",
        "    axes[0,0].set_ylabel('Loss')\n",
        "    axes[0,0].legend()\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Accuracy curves\n",
        "    axes[0,1].plot(epochs, train_accuracies, 'b-', label='Train Accuracy', linewidth=2)\n",
        "    axes[0,1].plot(epochs, val_accuracies, 'r-', label='Validation Accuracy', linewidth=2)\n",
        "    axes[0,1].set_title('Training and Validation Accuracy', fontweight='bold')\n",
        "    axes[0,1].set_xlabel('Epoch')\n",
        "    axes[0,1].set_ylabel('Accuracy (%)')\n",
        "    axes[0,1].legend()\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Learning rate\n",
        "    axes[1,0].plot(epochs, learning_rates, 'g-', linewidth=2)\n",
        "    axes[1,0].set_title('CosineAnnealingLR Schedule', fontweight='bold')\n",
        "    axes[1,0].set_xlabel('Epoch')\n",
        "    axes[1,0].set_ylabel('Learning Rate')\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Overfitting indicator\n",
        "    loss_diff = np.array(val_losses) - np.array(train_losses)\n",
        "    axes[1,1].plot(epochs, loss_diff, 'purple', linewidth=2)\n",
        "    axes[1,1].set_title('Overfitting Indicator (Val Loss - Train Loss)', fontweight='bold')\n",
        "    axes[1,1].set_xlabel('Epoch')\n",
        "    axes[1,1].set_ylabel('Loss Difference')\n",
        "    axes[1,1].grid(True, alpha=0.3)\n",
        "    axes[1,1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot training history\n",
        "plot_training_history(train_losses, val_losses, train_accuracies, val_accuracies, learning_rates)\n",
        "\n",
        "# ========== CELL 18: Test Set Evaluation ==========\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load('/content/best_efficientnet_model.pth'))\n",
        "print(\"âœ… Best model loaded for testing\")\n",
        "\n",
        "# Test evaluation\n",
        "test_loss, test_acc, test_preds, test_targets, test_probs = validate_epoch(model, test_loader, criterion, device)\n",
        "\n",
        "print(\"ðŸŽ¯ Test Set Results:\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "# ========== CELL 19: Confusion Matrix ==========\n",
        "def plot_confusion_matrix(y_true, y_pred, label_names):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Absolute confusion matrix\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "               xticklabels=label_names, yticklabels=label_names)\n",
        "    plt.title('Confusion Matrix (Absolute)', fontweight='bold')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "\n",
        "    # Normalized confusion matrix\n",
        "    plt.subplot(1, 2, 2)\n",
        "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues',\n",
        "               xticklabels=label_names, yticklabels=label_names)\n",
        "    plt.title('Confusion Matrix (Normalized)', fontweight='bold')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(test_targets, test_preds, label_names)\n",
        "\n",
        "# ========== CELL 20: ROC Curves ==========\n",
        "def plot_roc_curves(y_true, y_probs, label_names):\n",
        "    from sklearn.preprocessing import label_binarize\n",
        "\n",
        "    y_true_bin = label_binarize(y_true, classes=range(len(label_names)))\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    colors = plt.cm.Set1(np.linspace(0, 1, len(label_names)))\n",
        "\n",
        "    for i in range(len(label_names)):\n",
        "        if len(np.unique(y_true_bin[:, i])) > 1:\n",
        "            fpr, tpr, _ = roc_curve(y_true_bin[:, i], np.array(y_probs)[:, i])\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "            plt.plot(fpr, tpr, color=colors[i], lw=2,\n",
        "                    label=f'{label_names[i]} (AUC = {roc_auc:.3f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Multi-class ROC Curves - EfficientNet Model', fontweight='bold')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "# Plot ROC curves\n",
        "plot_roc_curves(test_targets, test_probs, label_names)\n",
        "\n",
        "# ========== CELL 21: Classification Report ==========\n",
        "print(\"ðŸ“Š Detailed Classification Report:\")\n",
        "print(\"=\" * 60)\n",
        "report = classification_report(test_targets, test_preds, target_names=label_names, digits=3)\n",
        "print(report)\n",
        "\n",
        "# Per-class metrics\n",
        "report_dict = classification_report(test_targets, test_preds, target_names=label_names, output_dict=True)\n",
        "\n",
        "print(\"\\nðŸ“ˆ Per-Class Detailed Metrics:\")\n",
        "print(\"-\" * 60)\n",
        "for class_name in label_names:\n",
        "    if class_name in report_dict:\n",
        "        precision = report_dict[class_name]['precision']\n",
        "        recall = report_dict[class_name]['recall']\n",
        "        f1 = report_dict[class_name]['f1-score']\n",
        "        support = report_dict[class_name]['support']\n",
        "        print(f\"{class_name:15} | Precision: {precision:.3f} | Recall: {recall:.3f} | \"\n",
        "              f\"F1: {f1:.3f} | Support: {support}\")\n",
        "\n",
        "print(f"
      ]
    }
  ]
}